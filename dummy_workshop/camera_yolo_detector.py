#!/usr/bin/env python3
"""
æœºæ¢°è‡‚è™šæ‹Ÿç›¸æœº + YOLO-E ç›®æ ‡æ£€æµ‹é›†æˆç¨‹åº
åŠŸèƒ½ï¼š
- é›†æˆè™šæ‹Ÿç›¸æœºæ¨¡æ‹Ÿå™¨çš„æ‰€æœ‰åŠŸèƒ½
- æ·»åŠ åŸºäºå‘½ä»¤çš„ç›®æ ‡æ£€æµ‹åŠŸèƒ½
- æ”¯æŒç”¨æˆ·è¾“å…¥æ–‡æœ¬æè¿°è¿›è¡Œç›®æ ‡æ£€æµ‹
- æ˜¾ç¤ºæ£€æµ‹ç»“æœå’Œæ©ç å¯è§†åŒ–
"""

import sys
import os
import math
import time
import threading
import numpy as np
import cv2
from PIL import Image
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt

# æ·»åŠ CLI-Toolè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), "CLI-Tool"))

import pybullet as p
import pybullet_data
from ultralytics import YOLO

class VirtualCameraYOLODetector:
    """è™šæ‹Ÿç›¸æœº + YOLO-E ç›®æ ‡æ£€æµ‹å™¨"""
    
    def __init__(self):
        # PyBulletä»¿çœŸç¯å¢ƒç›¸å…³
        self.physics_client = None
        self.robot_id = None
        self.joint_indices = []
        self.joint_names = ['joint1', 'joint2', 'joint3', 'joint4', 'joint5', 'joint6']
        
        # æœºæ¢°è‡‚çŠ¶æ€å®šä¹‰
        self.disabled_state = [0.0, 70.0, 90.0, 0.0, 0.0, 0.0]
        self.ready_state = [0.0, 60.0, 60.0, 90.0, 0.0, 0.0]
        self.current_joint_angles = self.disabled_state.copy()
        
        # ä½¿èƒ½å‚æ•°
        self.enable_duration = 3.0
        self.enable_steps = 120
        self.is_enabled = False
        
        # ç›¸æœºå‚æ•°
        self.camera_offset = [0.0, 0.06, 0.0]
        self.camera_fov = 120.0
        self.camera_aspect = 1.0
        self.camera_near = 0.01
        self.camera_far = 2.0
        self.image_width = 640
        self.image_height = 480
        
        # YOLOæ¨¡å‹
        self.yolo_model = None
        self.model_path = "yoloe-11s-seg.pt"
        
        # æ§åˆ¶å‚æ•°
        self.angle_step = 5.0
        self.selected_joint = 0
        self.end_effector_link_index = 7
        
        # è¿è¡Œæ§åˆ¶
        self.running = True
        self.camera_running = False
        
        print("ğŸ¤– æœºæ¢°è‡‚è™šæ‹Ÿç›¸æœº + YOLO-E ç›®æ ‡æ£€æµ‹å™¨")
        print("=" * 60)
    
    def load_yolo_model(self):
        """åŠ è½½YOLO-Eæ¨¡å‹"""
        try:
            model_full_path = os.path.join(os.path.dirname(__file__), self.model_path)
            if not os.path.exists(model_full_path):
                print(f"âŒ æ‰¾ä¸åˆ°YOLOæ¨¡å‹æ–‡ä»¶: {model_full_path}")
                return False
            
            print(f"ğŸ”„ åŠ è½½YOLO-Eæ¨¡å‹: {model_full_path}")
            self.yolo_model = YOLO(model_full_path, task='segment')
            print("âœ… YOLO-Eæ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ YOLOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def start_pybullet(self):
        """å¯åŠ¨PyBulletä»¿çœŸç¯å¢ƒ"""
        self.physics_client = p.connect(p.GUI)
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        p.setGravity(0, 0, -9.81)
        p.setPhysicsEngineParameter(enableFileCaching=0)
        
        # åŠ è½½åœ°é¢å’Œæœºæ¢°è‡‚
        p.loadURDF("plane.urdf", [0, 0, 0])
        
        urdf_path = os.path.join(os.path.dirname(os.path.dirname(
            os.path.realpath(__file__))), "dummy2", "dummy2.urdf")
        
        if not os.path.exists(urdf_path):
            print(f"âŒ æ‰¾ä¸åˆ°URDFæ–‡ä»¶: {urdf_path}")
            return False
        
        self.robot_id = p.loadURDF(urdf_path, [0, 0, 0], useFixedBase=True)
        self.get_joint_info()
        
        # è®¾ç½®è§†è§’
        p.resetDebugVisualizerCamera(
            cameraDistance=1.5, cameraYaw=45, cameraPitch=-30, cameraTargetPosition=[0, 0, 0.5])
        
        self.add_test_objects()
        return True
    
    def get_joint_info(self):
        """è·å–æœºæ¢°è‡‚å…³èŠ‚ä¿¡æ¯"""
        num_joints = p.getNumJoints(self.robot_id)
        self.joint_indices = []
        
        for i in range(num_joints):
            joint_info = p.getJointInfo(self.robot_id, i)
            joint_name = joint_info[1].decode('utf-8')
            
            if joint_name in self.joint_names:
                joint_index = self.joint_names.index(joint_name)
                if joint_index < len(self.joint_indices):
                    self.joint_indices[joint_index] = i
                else:
                    while len(self.joint_indices) <= joint_index:
                        self.joint_indices.append(-1)
                    self.joint_indices[joint_index] = i
    
    def add_test_objects(self):
        """æ·»åŠ æµ‹è¯•ç‰©ä½“"""
        # è´§æ¶ç»“æ„
        shelf_base_pos = [0.0, 0.4, 0.0]
        shelf_color = [0.8, 0.8, 0.8, 1.0]
        
        # è´§æ¶ç»„ä»¶
        components = [
            ([0.15, 0.05, 0.01], [0, 0, 0.01]),  # åº•åº§
            ([0.01, 0.01, 0.2], [-0.14, 0, 0.2]),  # å·¦ç«‹æŸ±
            ([0.01, 0.01, 0.2], [0.14, 0, 0.2]),   # å³ç«‹æŸ±
            ([0.14, 0.04, 0.005], [0, 0, 0.15]),   # ç¬¬ä¸€å±‚
            ([0.14, 0.04, 0.005], [0, 0, 0.3])     # ç¬¬äºŒå±‚
        ]
        
        for extents, offset in components:
            pos = [shelf_base_pos[0] + offset[0], shelf_base_pos[1] + offset[1], shelf_base_pos[2] + offset[2]]
            p.createMultiBody(
                baseMass=0,
                baseCollisionShapeIndex=p.createCollisionShape(p.GEOM_BOX, halfExtents=extents),
                baseVisualShapeIndex=p.createVisualShape(p.GEOM_BOX, halfExtents=extents, rgbaColor=shelf_color),
                basePosition=pos
            )
        
        # è´§æ¶ä¸Šçš„ç‰©å“
        item_colors = [[1, 0, 0, 1], [0, 1, 0, 1], [0, 0, 1, 1]]
        item_positions = [
            [shelf_base_pos[0] - 0.08, shelf_base_pos[1], shelf_base_pos[2] + 0.17],
            [shelf_base_pos[0], shelf_base_pos[1], shelf_base_pos[2] + 0.17],
            [shelf_base_pos[0] + 0.08, shelf_base_pos[1], shelf_base_pos[2] + 0.17]
        ]
        
        for pos, color in zip(item_positions, item_colors):
            p.createMultiBody(
                baseMass=0.05,
                baseCollisionShapeIndex=p.createCollisionShape(p.GEOM_BOX, halfExtents=[0.02, 0.02, 0.02]),
                baseVisualShapeIndex=p.createVisualShape(p.GEOM_BOX, halfExtents=[0.02, 0.02, 0.02], rgbaColor=color),
                basePosition=pos
            )
        
        print("âœ… æµ‹è¯•åœºæ™¯æ·»åŠ å®Œæˆ")
    
    def execute_enable_process(self):
        """æ‰§è¡Œä½¿èƒ½è¿‡ç¨‹"""
        print(f"ğŸ”„ ä½¿èƒ½è¿‡ç¨‹å¼€å§‹...")
        
        for step in range(self.enable_steps + 1):
            if not self.running:
                break
                
            progress = step / self.enable_steps
            smooth_progress = progress * progress * (3.0 - 2.0 * progress)
            
            self.current_joint_angles = [
                start + (end - start) * smooth_progress 
                for start, end in zip(self.disabled_state, self.ready_state)
            ]
            
            self.update_robot_pose()
            p.stepSimulation()
            time.sleep(self.enable_duration / self.enable_steps)
        
        self.is_enabled = True
        print("âœ… ä½¿èƒ½å®Œæˆ")
    
    def update_robot_pose(self):
        """æ›´æ–°æœºæ¢°è‡‚å§¿æ€"""
        for i, joint_idx in enumerate(self.joint_indices):
            if joint_idx != -1:
                p.resetJointState(self.robot_id, joint_idx, math.radians(self.current_joint_angles[i]))
    
    def capture_current_frame(self):
        """æ•è·å½“å‰ç›¸æœºç”»é¢"""
        if not self.is_enabled:
            print("âŒ è¯·å…ˆå®Œæˆä½¿èƒ½è¿‡ç¨‹")
            return None, None
        
        self.update_robot_pose()
        p.stepSimulation()
        
        # è·å–æœ«ç«¯ä½ç½®
        link_state = p.getLinkState(self.robot_id, self.end_effector_link_index, computeForwardKinematics=True)
        flange_position = list(link_state[0])
        flange_orientation = list(link_state[1])
        
        # è®¡ç®—ç›¸æœºä½ç½®
        rotation_matrix = np.array(p.getMatrixFromQuaternion(flange_orientation)).reshape(3, 3)
        camera_offset_global = rotation_matrix @ np.array(self.camera_offset)
        camera_position = np.array(flange_position) + camera_offset_global
        forward_direction = rotation_matrix @ np.array([0, 1, 0])
        camera_target = camera_position + forward_direction * 0.2
        
        # æ¸²æŸ“å›¾åƒ
        view_matrix = p.computeViewMatrix(camera_position, camera_target, [0, 0, 1])
        projection_matrix = p.computeProjectionMatrixFOV(
            fov=self.camera_fov, aspect=self.camera_aspect, 
            nearVal=self.camera_near, farVal=self.camera_far
        )
        
        width, height, rgb_img, depth_img, seg_img = p.getCameraImage(
            width=self.image_width, height=self.image_height,
            viewMatrix=view_matrix, projectionMatrix=projection_matrix,
            renderer=p.ER_BULLET_HARDWARE_OPENGL
        )
        
        # è½¬æ¢æ ¼å¼
        rgb_array = np.array(rgb_img).reshape(height, width, 4)[:, :, :3]
        bgr_array = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2BGR)
        
        return bgr_array, rgb_array
    
    def perform_yolo_detection(self, rgb_image, text_prompt):
        """æ‰§è¡ŒYOLOæ£€æµ‹"""
        if self.yolo_model is None:
            print("âŒ YOLOæ¨¡å‹æœªåŠ è½½")
            return None
        
        try:
            print(f"ğŸ” æ­£åœ¨æ£€æµ‹: '{text_prompt}'")
            pil_image = Image.fromarray(rgb_image)
            
            # è®¾ç½®æ£€æµ‹ç±»åˆ«
            self.yolo_model.set_classes([text_prompt], self.yolo_model.get_text_pe([text_prompt]))
            results = self.yolo_model.predict(pil_image, conf=0.5)
            
            detected_count = len(results[0].boxes) if results[0].boxes is not None else 0
            print(f"âœ… æ£€æµ‹å®Œæˆï¼Œå‘ç° {detected_count} ä¸ªç›®æ ‡")
            return results
            
        except Exception as e:
            print(f"âŒ YOLOæ£€æµ‹å¤±è´¥: {e}")
            return None
    
    def visualize_detection_results(self, image, results, text_prompt):
        """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        if not results or results[0].boxes is None:
            print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°ç›®æ ‡")
            return
        
        # æ£€æµ‹ç»“æœå¤„ç†
        result_image = image.copy()
        boxes = results[0].boxes.xyxy.cpu().numpy()
        confidences = results[0].boxes.conf.cpu().numpy()
        
        print(f"ğŸ“Š æ£€æµ‹ç»Ÿè®¡:")
        print(f"   å‘ç°ç›®æ ‡æ•°é‡: {len(boxes)}")
        
        # å¤„ç†æ©ç 
        if hasattr(results[0], 'masks') and results[0].masks is not None:
            masks = results[0].masks.data.cpu().numpy()
            print(f"   åŒ…å«åˆ†å‰²æ©ç : æ˜¯")
            
            for i, (box, conf, mask) in enumerate(zip(boxes, confidences, masks)):
                x1, y1, x2, y2 = box.astype(int)
                
                # è°ƒæ•´æ©ç å°ºå¯¸
                mask_resized = cv2.resize(mask, (image.shape[1], image.shape[0]))
                mask_binary = (mask_resized > 0.5).astype(np.uint8)
                
                # åˆ›å»ºå½©è‰²æ©ç 
                color = np.random.randint(0, 255, 3).tolist()
                colored_mask = np.zeros_like(result_image)
                colored_mask[mask_binary == 1] = color
                
                # å åŠ æ©ç 
                result_image = cv2.addWeighted(result_image, 0.7, colored_mask, 0.3, 0)
                
                # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ ‡ç­¾
                cv2.rectangle(result_image, (x1, y1), (x2, y2), color, 2)
                label = f"{text_prompt} {conf:.2f}"
                cv2.putText(result_image, label, (x1, y1 - 5), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                print(f"   ç›®æ ‡ {i+1}: ç½®ä¿¡åº¦ {conf:.3f}, ä½ç½® ({x1},{y1})-({x2},{y2})")
        else:
            print(f"   åŒ…å«åˆ†å‰²æ©ç : å¦")
            # ä»…æ£€æµ‹æ¡†
            for i, (box, conf) in enumerate(zip(boxes, confidences)):
                x1, y1, x2, y2 = box.astype(int)
                color = (0, 255, 0)
                cv2.rectangle(result_image, (x1, y1), (x2, y2), color, 2)
                label = f"{text_prompt} {conf:.2f}"
                cv2.putText(result_image, label, (x1, y1 - 5), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
                print(f"   ç›®æ ‡ {i+1}: ç½®ä¿¡åº¦ {conf:.3f}, ä½ç½® ({x1},{y1})-({x2},{y2})")
        
        # ä¿å­˜ç»“æœå›¾åƒ
        timestamp = int(time.time())
        result_filename = f"detection_result_{timestamp}.png"
        result_path = os.path.join(os.path.dirname(__file__), result_filename)
        cv2.imwrite(result_path, result_image)
        print(f"ğŸ“¸ æ£€æµ‹ç»“æœå·²ä¿å­˜: {result_path}")
        
        # ä½¿ç”¨OpenCVæ˜¾ç¤ºç»“æœï¼ˆæ›´ç¨³å®šï¼‰
        try:
            # åˆ›å»ºå¯¹æ¯”å›¾åƒ
            original_resized = cv2.resize(image, (320, 240))
            result_resized = cv2.resize(result_image, (320, 240))
            
            # æ°´å¹³æ‹¼æ¥
            comparison = np.hstack([original_resized, result_resized])
            
            # æ·»åŠ æ ‡é¢˜
            cv2.putText(comparison, "Original", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(comparison, f"Detection: {len(boxes)} targets", (330, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # æ˜¾ç¤ºå¯¹æ¯”å›¾åƒ
            cv2.imshow('Detection Results', comparison)
            print("ğŸ–¼ï¸  æ£€æµ‹ç»“æœæ˜¾ç¤ºçª—å£å·²æ‰“å¼€ï¼ŒæŒ‰ä»»æ„é”®å…³é—­")
            cv2.waitKey(0)
            cv2.destroyWindow('Detection Results')
            
        except Exception as e:
            print(f"âš ï¸  å›¾åƒæ˜¾ç¤ºå¤±è´¥: {e}")
            print("ğŸ“ è¯·æŸ¥çœ‹ä¿å­˜çš„ç»“æœå›¾åƒæ–‡ä»¶")
    
    def camera_thread(self):
        """ç›¸æœºæ˜¾ç¤ºçº¿ç¨‹"""
        cv2.namedWindow('Virtual Camera View', cv2.WINDOW_AUTOSIZE)
        
        while self.running and self.camera_running:
            self.update_robot_pose()
            p.stepSimulation()
            
            bgr_image, _ = self.capture_current_frame()
            if bgr_image is not None:
                # æ·»åŠ çŠ¶æ€ä¿¡æ¯
                display_image = bgr_image.copy()
                cv2.putText(display_image, f"çŠ¶æ€: {'å·²ä½¿èƒ½' if self.is_enabled else 'æœªä½¿èƒ½'}", 
                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                cv2.putText(display_image, f"è¾“å…¥ 'detect <æè¿°>' è¿›è¡Œæ£€æµ‹", 
                          (10, display_image.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                
                cv2.imshow('Virtual Camera View', display_image)
            
            cv2.waitKey(1)
            time.sleep(0.03)
        
        cv2.destroyAllWindows()
    
    def handle_terminal_input(self, command):
        """å¤„ç†ç»ˆç«¯å‘½ä»¤"""
        command = command.strip()
        
        if command.lower() in ['quit', 'exit']:
            return False
        
        elif command.lower().startswith('joint '):
            try:
                joint_num = int(command.split()[1])
                if 1 <= joint_num <= 6:
                    self.selected_joint = joint_num - 1
                    print(f"ğŸ¯ é€‰ä¸­å…³èŠ‚ J{joint_num}")
                else:
                    print("âŒ å…³èŠ‚ç¼–å·å¿…é¡»åœ¨1-6ä¹‹é—´")
            except:
                print("âŒ è¯·è¾“å…¥æ­£ç¡®çš„å…³èŠ‚ç¼–å·")
        
        elif command.lower() == 'up':
            if self.is_enabled:
                self.current_joint_angles[self.selected_joint] += self.angle_step
                print(f"â¬†ï¸ J{self.selected_joint + 1}: {self.current_joint_angles[self.selected_joint]:.1f}Â°")
            else:
                print("âŒ è¯·å…ˆå®Œæˆä½¿èƒ½è¿‡ç¨‹")
        
        elif command.lower() == 'down':
            if self.is_enabled:
                self.current_joint_angles[self.selected_joint] -= self.angle_step
                print(f"â¬‡ï¸ J{self.selected_joint + 1}: {self.current_joint_angles[self.selected_joint]:.1f}Â°")
            else:
                print("âŒ è¯·å…ˆå®Œæˆä½¿èƒ½è¿‡ç¨‹")
        
        elif command.lower().startswith('detect '):
            text_prompt = command[7:].strip()
            if not text_prompt:
                print("âŒ è¯·æä¾›æ£€æµ‹ç›®æ ‡çš„æ–‡æœ¬æè¿°")
                return True
            
            print("ğŸ“¹ æ•è·å½“å‰ç”»é¢...")
            bgr_image, rgb_image = self.capture_current_frame()
            
            if bgr_image is None:
                return True
            
            print("ğŸ¤– å¼€å§‹YOLO-Eæ£€æµ‹...")
            results = self.perform_yolo_detection(rgb_image, text_prompt)
            
            if results:
                self.visualize_detection_results(bgr_image, results, text_prompt)
        
        elif command.lower() == 'capture':
            print("ğŸ“¹ æ•è·å½“å‰ç”»é¢...")
            bgr_image, rgb_image = self.capture_current_frame()
            
            if bgr_image is not None:
                # ä¿å­˜å›¾åƒ
                timestamp = int(time.time())
                capture_path = os.path.join(os.path.dirname(__file__), f"captured_frame_{timestamp}.png")
                cv2.imwrite(capture_path, bgr_image)
                print(f"ğŸ“¸ ç”»é¢å·²ä¿å­˜: {capture_path}")
                
                # ä½¿ç”¨OpenCVæ˜¾ç¤ºï¼ˆæ›´ç¨³å®šï¼‰
                try:
                    cv2.imshow('Captured Frame', bgr_image)
                    print("ğŸ–¼ï¸  æ•è·ç”»é¢æ˜¾ç¤ºçª—å£å·²æ‰“å¼€ï¼ŒæŒ‰ä»»æ„é”®å…³é—­")
                    cv2.waitKey(0)
                    cv2.destroyWindow('Captured Frame')
                except Exception as e:
                    print(f"âš ï¸  å›¾åƒæ˜¾ç¤ºå¤±è´¥: {e}")
                    print("ğŸ“ è¯·æŸ¥çœ‹ä¿å­˜çš„å›¾åƒæ–‡ä»¶")
        
        elif command.lower() == 'reset':
            if self.is_enabled:
                self.current_joint_angles = self.ready_state.copy()
                print("ğŸ”„ é‡ç½®åˆ°ä½¿èƒ½çŠ¶æ€")
            else:
                print("âŒ è¯·å…ˆå®Œæˆä½¿èƒ½è¿‡ç¨‹")
        
        elif command.lower() == 'help':
            print("ğŸ“– å¯ç”¨å‘½ä»¤:")
            print("   joint <1-6>           - é€‰æ‹©å…³èŠ‚")
            print("   up/down               - è°ƒæ•´å…³èŠ‚è§’åº¦")
            print("   detect <æ–‡æœ¬æè¿°>     - ç›®æ ‡æ£€æµ‹")
            print("   capture               - æ•è·ç”»é¢")
            print("   reset                 - é‡ç½®å§¿æ€")
            print("   help                  - æ˜¾ç¤ºå¸®åŠ©")
            print("   quit                  - é€€å‡ºç¨‹åº")
            print("\nğŸ“ æ£€æµ‹ç¤ºä¾‹:")
            print("   detect red cube")
            print("   detect yellow object")
            print("   detect box on shelf")
        
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤ï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
        
        return True
    
    def run_simulation(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        print("\nğŸš€ å¯åŠ¨ç¨‹åº...")
        
        if not self.load_yolo_model():
            return
        
        # åˆå§‹è®¾ç½®
        for i, joint_idx in enumerate(self.joint_indices):
            if joint_idx != -1:
                p.resetJointState(self.robot_id, joint_idx, math.radians(self.disabled_state[i]))
        
        input("æŒ‰ Enter å¼€å§‹ä½¿èƒ½è¿‡ç¨‹...")
        self.execute_enable_process()
        
        if not self.running:
            return
        
        print("\nğŸ“¹ å¯åŠ¨è™šæ‹Ÿç›¸æœº...")
        self.camera_running = True
        camera_thread = threading.Thread(target=self.camera_thread)
        camera_thread.daemon = True
        camera_thread.start()
        
        print("\nğŸ’¬ ç»ˆç«¯äº¤äº’æ¨¡å¼å¯åŠ¨")
        print("è¾“å…¥ 'help' æŸ¥çœ‹å‘½ä»¤ï¼Œè¾“å…¥ 'detect <æè¿°>' è¿›è¡Œæ£€æµ‹")
        print("=" * 60)
        
        while self.running:
            command = input("è¯·è¾“å…¥å‘½ä»¤: ")
            if not self.handle_terminal_input(command):
                break
        
        print("ğŸ”š ç¨‹åºç»“æŸ")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.running = False
        self.camera_running = False
        if self.physics_client is not None:
            p.disconnect()

def main():
    """ä¸»å‡½æ•°"""
    detector = VirtualCameraYOLODetector()
    
    if not detector.start_pybullet():
        return
    
    detector.run_simulation()
    detector.cleanup()

if __name__ == "__main__":
    main() 